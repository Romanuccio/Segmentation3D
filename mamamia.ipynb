{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c8ddc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-30 14:23:02.453239: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-30 14:23:02.457518: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-30 14:23:02.466441: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751286182.480892  389517 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751286182.484475  389517 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1751286182.495809  389517 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751286182.495824  389517 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751286182.495826  389517 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751286182.495827  389517 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-30 14:23:02.500838: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import wandb\n",
    "from models.lightning_models import Unet3D\n",
    "from modules.segmentation_model import Unet, PositionalUnet\n",
    "from loaders.lazy_loaders import PatchDataloader\n",
    "from monai.losses.dice import DiceLoss\n",
    "from losses.jaccard_loss import JaccardLoss # TODO for later\n",
    "from torch.nn import BCELoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "# from losses.dice_loss import DiceLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487b1b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_list = [2]\n",
    "# slices = 80\n",
    "\n",
    "train = np.load(\"../MAMAMIA_Challenge/train_ids_v_pojeb.npy\", allow_pickle=True)\n",
    "validation = np.load(\"../MAMAMIA_Challenge/validation_ids_v_pojeb.npy\", allow_pickle=True)[:200]\n",
    "sets = [train, validation]\n",
    "\n",
    "# get zipped filepaths depending on channel count\n",
    "channel_images = [None] * len(sets)\n",
    "for i, set in enumerate(sets):\n",
    "    set_images = [None] * len(channel_list)\n",
    "    for j, channel in enumerate(channel_list):\n",
    "        collected_channel_list = []\n",
    "        for set_id in set:\n",
    "            set_channel = next(set_id.glob(f\"*000{channel}.nii.gz\"))\n",
    "            collected_channel_list.append(set_channel)\n",
    "\n",
    "        set_images[j] = collected_channel_list\n",
    "\n",
    "    channel_images[i] = set_images\n",
    "\n",
    "# get labels\n",
    "labels = [None] * len(sets)\n",
    "for i, set in enumerate(sets):\n",
    "    set_labels = []\n",
    "    for id in set:\n",
    "        seg = next(id.glob(\"*segmentation.nii.gz\"))\n",
    "        set_labels.append(seg)\n",
    "    labels[i] = set_labels\n",
    "\n",
    "# get dicts\n",
    "train_dicts = [\n",
    "    {\"image\": image_name, \"label\": label_name}\n",
    "    for image_name, label_name in zip(list(zip(*channel_images[0])), labels[0])\n",
    "]\n",
    "val_dicts = [\n",
    "    {\"image\": image_name, \"label\": label_name}\n",
    "    for image_name, label_name in zip(list(zip(*channel_images[1])), labels[1])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3ad5843",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = (64, 64, 64)\n",
    "train_dataset = PatchDataloader(images_labels_dict=train_dicts, repeat=4, patch_size=patch_size)\n",
    "validation_dataset = PatchDataloader(images_labels_dict=val_dicts, repeat=1, patch_size=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7afc0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64, 64, 64]),\n",
       " torch.Size([64, 64, 64]),\n",
       " torch.Size([1, 154, 146, 161]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].shape, train_dataset[0][1].shape, validation_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f712aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# dat = train_dataset[12]\n",
    "# plt.imshow(dat[0][0,30,...], cmap='gray')\n",
    "# plt.show()\n",
    "# plt.imshow(dat[1][30,...], cmap='gray')\n",
    "# plt.show()\n",
    "# # TODO extract 3D images and check in 3dslicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad1ab922",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False, num_workers=16)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=min(slices, 16), shuffle=False, num_workers=min(slices, 16))\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=1, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc01dce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = (32, 32, 32)\n",
    "model = Unet3D(\n",
    "    model=Unet,\n",
    "    depths=[3, 3, 3, 3],\n",
    "    # loss=BCELoss(),\n",
    "    # loss=JaccardLoss(from_logits=True, reduce=True),\n",
    "    # loss=DiceLoss(convert_logits_to_probs=True),\n",
    "    loss=DiceLoss(include_background=True, to_onehot_y=True, sigmoid=True),\n",
    "    patch_size=patch_size,\n",
    "    strides=stride,\n",
    "    padding=\"same\",\n",
    "    classes=1,\n",
    "    beta=1,\n",
    "    initial_LR=1e-4\n",
    "    # final_activation=torch.nn.Sigmoid(),\n",
    "    # optimizer=Adam\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f451706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda:0')\n",
    "# checkpoint_path = (\n",
    "#     \"/home/romanuccio/RomanuccioDiff/Segmentation3D/Gianluca_Mamamia/3mkossiu/checkpoints/best-checkpoint-epoch=11-val_loss=0.91.ckpt\"\n",
    "# )\n",
    "# model = Unet3D.load_from_checkpoint(\n",
    "#     checkpoint_path,\n",
    "#     model=Unet,\n",
    "#     depths=[3, 3, 3, 3],\n",
    "#     # loss=BCELoss(),\n",
    "#     # loss=JaccardLoss(from_logits=True, reduce=True),\n",
    "#     loss=DiceLoss(convert_logits_to_probs=True),\n",
    "#     # loss=DiceLoss(include_background=True, to_onehot_y=False, sigmoid=True),\n",
    "#     patch_size=patch_size,\n",
    "#     strides=stride,\n",
    "#     padding=\"same\",\n",
    "#     classes=1,\n",
    "#     beta=1,\n",
    "# )\n",
    "# # model.eval()  # or model.train() depending on your use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cf9d061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = []\n",
    "# for i, val_data in enumerate(validation_loader):\n",
    "#     if i > 1:\n",
    "#         break\n",
    "#     # model.validation_step(val_data, i)\n",
    "#     # image, label = val_data\n",
    "#     # print(image.shape)\n",
    "#     # # model.predict_step(\n",
    "#     # #     image,\n",
    "#     # #     patch_size=patch_size,\n",
    "#     # #     strides=stride,\n",
    "#     # #     padding=\"same\", unpad=True, verbose=True\n",
    "#     # # )\n",
    "    \n",
    "#     data.append(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b16fb1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# asdf = data[0].to(device)\n",
    "# model.validation_step(asdf, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38af261a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd287e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX A6000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mromanuccio\u001b[0m (\u001b[33mromanuccio-brno-university-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250630_142305-c1ucpswg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/romanuccio-brno-university-of-technology/Gianluca_Mamamia/runs/c1ucpswg' target=\"_blank\">hardy-oath-77</a></strong> to <a href='https://wandb.ai/romanuccio-brno-university-of-technology/Gianluca_Mamamia' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/romanuccio-brno-university-of-technology/Gianluca_Mamamia' target=\"_blank\">https://wandb.ai/romanuccio-brno-university-of-technology/Gianluca_Mamamia</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/romanuccio-brno-university-of-technology/Gianluca_Mamamia/runs/c1ucpswg' target=\"_blank\">https://wandb.ai/romanuccio-brno-university-of-technology/Gianluca_Mamamia/runs/c1ucpswg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name             | Type     | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | final_activation | Identity | 0      | train\n",
      "1 | model            | Unet     | 2.0 M  | train\n",
      "2 | loss             | DiceLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "2.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 M     Total params\n",
      "8.089     Total estimated model params size (MB)\n",
      "287       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a16e613f16a4eb99cd26cb2194619b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
      "single channel prediction, `to_onehot_y=True` ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c46c5f5b894793aa9eb59caf45f217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9853b3213e4d4770b67f71ff0f87069f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.trainer import Trainer\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",         # Metric to monitor\n",
    "    mode=\"min\",                 # 'min' for loss, 'max' for accuracy\n",
    "    save_top_k=1,               # Save only the best model\n",
    "    filename=\"best-checkpoint-{epoch:02d}-{val_loss:.2f}\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"Gianluca_Mamamia\")\n",
    "trainer = Trainer(max_epochs=10, callbacks=[checkpoint_callback], devices=[1], log_every_n_steps=0, logger=wandb_logger)\n",
    "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09494403",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
